#!/usr/bin/env python3
"""
Doxygen Documentation Crawler - GUI Version
Doxygenìœ¼ë¡œ ìƒì„±ëœ API ë¬¸ì„œ ì „ìš© í¬ë¡¤ëŸ¬
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, filedialog, messagebox
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
import time
import json
import os
from pathlib import Path
import threading
import re


class DoxygenCrawlerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Doxygen Documentation Crawler")
        self.root.geometry("900x700")
        
        self.crawler = None
        self.is_crawling = False
        
        self.setup_ui()
    
    def setup_ui(self):
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # URL Input
        ttk.Label(main_frame, text="ë¬¸ì„œ URL:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.url_entry = ttk.Entry(main_frame, width=70)
        self.url_entry.grid(row=0, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        self.url_entry.insert(0, "https://developer.nvidia.com/docs/drive/drive-os/6.0.10/public/drive-os-linux-sdk/api_reference/index.html")
        
        # Options
        options_frame = ttk.LabelFrame(main_frame, text="ì˜µì…˜", padding="10")
        options_frame.grid(row=1, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        
        ttk.Label(options_frame, text="ìµœëŒ€ í˜ì´ì§€ ìˆ˜:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.max_pages_var = tk.StringVar(value="500")
        ttk.Entry(options_frame, textvariable=self.max_pages_var, width=10).grid(row=0, column=1, sticky=tk.W, padx=5)
        
        ttk.Label(options_frame, text="ìš”ì²­ ê°„ê²©(ì´ˆ):").grid(row=0, column=2, sticky=tk.W, padx=5)
        self.delay_var = tk.StringVar(value="1.0")
        ttk.Entry(options_frame, textvariable=self.delay_var, width=10).grid(row=0, column=3, sticky=tk.W, padx=5)
        
        ttk.Label(options_frame, text="ì¶œë ¥ í´ë”:").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)
        self.output_dir_var = tk.StringVar(value="doxygen_crawl")
        ttk.Entry(options_frame, textvariable=self.output_dir_var, width=30).grid(row=1, column=1, columnspan=2, sticky=(tk.W, tk.E), padx=5, pady=5)
        ttk.Button(options_frame, text="ì°¾ì•„ë³´ê¸°", command=self.browse_output_dir).grid(row=1, column=3, padx=5, pady=5)
        
        # Buttons
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=2, column=0, columnspan=3, pady=10)
        
        self.start_button = ttk.Button(button_frame, text="í¬ë¡¤ë§ ì‹œì‘", command=self.start_crawl)
        self.start_button.grid(row=0, column=0, padx=5)
        
        self.stop_button = ttk.Button(button_frame, text="ì¤‘ì§€", command=self.stop_crawl, state=tk.DISABLED)
        self.stop_button.grid(row=0, column=1, padx=5)
        
        ttk.Button(button_frame, text="ê²°ê³¼ í´ë” ì—´ê¸°", command=self.open_output_folder).grid(row=0, column=2, padx=5)
        
        # Progress
        ttk.Label(main_frame, text="ì§„í–‰ ìƒí™©:").grid(row=3, column=0, sticky=tk.W, pady=5)
        self.progress_var = tk.StringVar(value="ëŒ€ê¸° ì¤‘...")
        ttk.Label(main_frame, textvariable=self.progress_var).grid(row=3, column=1, columnspan=2, sticky=tk.W, pady=5)
        
        self.progress_bar = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress_bar.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)
        
        # Log
        ttk.Label(main_frame, text="ë¡œê·¸:").grid(row=5, column=0, sticky=tk.W, pady=5)
        self.log_text = scrolledtext.ScrolledText(main_frame, width=80, height=20, wrap=tk.WORD)
        self.log_text.grid(row=6, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Configure grid
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(6, weight=1)
    
    def browse_output_dir(self):
        directory = filedialog.askdirectory()
        if directory:
            self.output_dir_var.set(directory)
    
    def open_output_folder(self):
        output_dir = self.output_dir_var.get()
        if os.path.exists(output_dir):
            os.startfile(output_dir)
        else:
            messagebox.showwarning("ê²½ê³ ", f"ì¶œë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {output_dir}")
    
    def log(self, message):
        self.log_text.insert(tk.END, f"{message}\n")
        self.log_text.see(tk.END)
        self.root.update_idletasks()
    
    def start_crawl(self):
        url = self.url_entry.get().strip()
        if not url:
            messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            max_pages = int(self.max_pages_var.get())
            delay = float(self.delay_var.get())
        except ValueError:
            messagebox.showerror("ì˜¤ë¥˜", "ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì™€ ìš”ì²­ ê°„ê²©ì€ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
            return
        
        self.is_crawling = True
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.progress_bar.start()
        self.log_text.delete(1.0, tk.END)
        
        thread = threading.Thread(
            target=self.run_crawl,
            args=(url, max_pages, delay),
            daemon=True
        )
        thread.start()
    
    def run_crawl(self, url, max_pages, delay):
        try:
            output_dir = self.output_dir_var.get()
            
            self.log(f"Doxygen ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘")
            self.log(f"URL: {url}")
            self.log(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}\n")
            
            self.crawler = DoxygenCrawler(
                url, max_pages, delay, output_dir,
                self.log, lambda: self.is_crawling
            )
            results = self.crawler.crawl()
            
            if self.is_crawling and results:
                self.log(f"\n{'='*60}")
                self.log("ê²°ê³¼ ì €ì¥ ì¤‘...")
                
                json_file = self.crawler.save_json()
                files_msg, summary_txt = self.crawler.save_txt()
                
                self.log(f"âœ“ JSON: {json_file}")
                self.log(f"âœ“ ì›ë¬¸ TXT: {files_msg}")
                self.log(f"âœ“ ìš”ì•½ TXT: {summary_txt}")
                self.log(f"{'='*60}\n")
                self.log(f"ì™„ë£Œ! ì´ {len(results)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")
                
                self.progress_var.set(f"ì™„ë£Œ! {len(results)}ê°œ í˜ì´ì§€")
                messagebox.showinfo("ì™„ë£Œ", f"í¬ë¡¤ë§ ì™„ë£Œ!\n{len(results)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")
            else:
                self.log("\nì¤‘ì§€ë¨")
                self.progress_var.set("ì¤‘ì§€ë¨")
        
        except Exception as e:
            self.log(f"\nì˜¤ë¥˜: {str(e)}")
            import traceback
            self.log(traceback.format_exc())
            self.progress_var.set("ì˜¤ë¥˜ ë°œìƒ")
            messagebox.showerror("ì˜¤ë¥˜", f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜:\n{str(e)}")
        
        finally:
            self.progress_bar.stop()
            self.start_button.config(state=tk.NORMAL)
            self.stop_button.config(state=tk.DISABLED)
            self.is_crawling = False
    
    def stop_crawl(self):
        self.is_crawling = False
        self.log("\nì¤‘ì§€ ìš”ì²­...")


class DoxygenCrawler:
    """Specialized crawler for Doxygen documentation"""
    
    # Common Doxygen file patterns
    DOXYGEN_PAGES = [
        'index.html',
        'modules.html',
        'namespaces.html',
        'classes.html',
        'files.html',
        'annotated.html',
        'functions.html',
        'globals.html',
        'pages.html',
    ]
    
    def __init__(self, base_url, max_pages, delay, output_dir, log_func, should_continue):
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.output_dir = output_dir
        self.log = log_func
        self.should_continue = should_continue
        self.visited_urls = set()
        self.pages_data = []
        
        parsed = urlparse(base_url)
        self.domain = f"{parsed.scheme}://{parsed.netloc}"
        self.base_path = '/'.join(parsed.path.split('/')[:-1]) + '/'
        
        self.log(f"ë„ë©”ì¸: {self.domain}")
        self.log(f"ê¸°ë³¸ ê²½ë¡œ: {self.base_path}")
        
        self.create_directories()
    
    def create_directories(self):
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.output_dir, "crawl_ì›ë¬¸").mkdir(parents=True, exist_ok=True)
        Path(self.output_dir, "crawl_ìš”ì•½ë³¸").mkdir(parents=True, exist_ok=True)
        Path(self.output_dir, "crawlJson").mkdir(parents=True, exist_ok=True)
    
    def is_valid_url(self, url):
        if not url or url in self.visited_urls:
            return False
        
        if url.startswith('#') or url.startswith('javascript:') or url.startswith('mailto:'):
            return False
        
        parsed = urlparse(url)
        if parsed.netloc and parsed.netloc != urlparse(self.domain).netloc:
            return False
        
        full_url = urljoin(self.domain, url)
        if not full_url.startswith(f"{self.domain}{self.base_path}"):
            return False
        
        # Must be HTML or PDF
        if not (full_url.endswith('.html') or full_url.endswith('.htm') or full_url.endswith('.pdf')):
            return False
        
        return True
    
    def find_all_html_files(self, soup, current_url):
        """Find all HTML file links in the page"""
        links = set()
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # Skip anchors
            if href.startswith('#'):
                continue
            
            full_url = urljoin(current_url, href)
            
            if self.is_valid_url(full_url):
                links.add(full_url)
        
        return list(links)
    
    def get_common_doxygen_pages(self):
        """Get URLs for common Doxygen index pages"""
        pages = []
        for page_name in self.DOXYGEN_PAGES:
            url = f"{self.domain}{self.base_path}{page_name}"
            pages.append(url)
        return pages
    
    def extract_pdf_text(self, pdf_content):
        """Extract text from PDF content"""
        try:
            import io
            try:
                from pypdf import PdfReader
            except ImportError:
                try:
                    from PyPDF2 import PdfReader
                except ImportError:
                    self.log("    âš ï¸  PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (pip install pypdf)")
                    return None
            
            pdf_file = io.BytesIO(pdf_content)
            reader = PdfReader(pdf_file)
            
            text = []
            for page_num, page in enumerate(reader.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text.append(f"[í˜ì´ì§€ {page_num}]\n{page_text}\n")
            
            return '\n'.join(text)
        
        except Exception as e:
            self.log(f"    âŒ PDF ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def extract_content(self, soup):
        content = {
            'title': '',
            'headings': [],
            'text': '',
            'code_blocks': []
        }
        
        # Extract title - prioritize h1 over title tag for Doxygen
        h1_tag = soup.find('h1')
        title_tag = soup.find('title')
        
        if h1_tag:
            content['title'] = h1_tag.get_text(strip=True)
        elif title_tag:
            content['title'] = title_tag.get_text(strip=True)
        
        # Doxygen usually has content in specific divs
        main_selectors = [
            '.contents',  # Doxygen main content class
            '#doc-content',
            'main',
            'article',
            '.textblock',
            'body'
        ]
        
        main = None
        for selector in main_selectors:
            main = soup.select_one(selector)
            if main:
                break
        
        if not main:
            main = soup.find('body')
        
        if main:
            # Headings
            for heading in main.find_all(['h1', 'h2', 'h3', 'h4']):
                content['headings'].append({
                    'level': heading.name,
                    'text': heading.get_text(strip=True)
                })
            
            # Code blocks
            for code in main.find_all(['code', 'pre', '.fragment']):
                code_text = code.get_text(strip=True)
                if len(code_text) > 10:
                    content['code_blocks'].append(code_text)
            
            # Remove unwanted elements
            for element in main(['script', 'style', 'nav', 'header', 'footer', '.navpath']):
                element.decompose()
            
            # Text
            content['text'] = main.get_text(separator='\n', strip=True)
        
        return content
    
    def crawl_page(self, url):
        self.log(f"  ì²˜ë¦¬: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            
            # Check if PDF
            if url.endswith('.pdf'):
                self.log(f"    ğŸ“„ PDF íŒŒì¼ ê°ì§€")
                pdf_text = self.extract_pdf_text(response.content)
                
                if pdf_text:
                    # Extract title from filename
                    title = url.split('/')[-1].replace('.pdf', '')
                    
                    self.log(f"    âœ“ PDF ë³€í™˜ ì™„ë£Œ: {title}")
                    
                    return {
                        'url': url,
                        'status': 'success',
                        'title': title,
                        'headings': [],
                        'text': pdf_text,
                        'code_blocks': [],
                        'file_type': 'pdf'
                    }
                else:
                    return {
                        'url': url,
                        'status': 'error',
                        'error': 'PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨',
                        'file_type': 'pdf'
                    }
            
            # HTML processing
            soup = BeautifulSoup(response.content, 'html.parser')
            content = self.extract_content(soup)
            
            # If title is generic or empty, use filename from URL
            title = content.get('title', '')
            if not title or title == 'NVIDIA DRIVE OS Linux SDK API Reference':
                # Extract filename from URL
                filename = url.split('/')[-1]
                if filename.endswith('.html'):
                    title = filename.replace('.html', '').replace('_', ' ')
                else:
                    title = filename
                content['title'] = title
            
            self.log(f"    âœ“ {content.get('title', 'Untitled')}")
            
            return {
                'url': url,
                'status': 'success',
                'soup': soup,  # Keep for link extraction
                'file_type': 'html',
                **content
            }
        
        except Exception as e:
            self.log(f"    âŒ {str(e)}")
            return {
                'url': url,
                'status': 'error',
                'error': str(e),
                'soup': None
            }
    
    def crawl(self):
        self.log(f"\n{'='*60}")
        self.log("1ë‹¨ê³„: ì‹œì‘ í˜ì´ì§€ ë° ê³µí†µ Doxygen í˜ì´ì§€ í™•ì¸")
        self.log(f"{'='*60}\n")
        
        # Start with common Doxygen pages
        seed_urls = self.get_common_doxygen_pages()
        
        # Add base URL if not already in list (avoid duplicates)
        if self.base_url not in seed_urls:
            seed_urls.insert(0, self.base_url)
        
        self.log(f"ì‹œë“œ URL {len(seed_urls)}ê°œ í™•ì¸ ì¤‘...")
        
        all_links = set()
        
        # Check seed URLs and collect links
        for url in seed_urls:
            if not self.should_continue():
                break
            
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, timeout=10, headers=headers)
                
                if response.status_code == 200:
                    self.log(f"  âœ“ ë°œê²¬: {url.split('/')[-1]}")
                    soup = BeautifulSoup(response.content, 'html.parser')
                    links = self.find_all_html_files(soup, url)
                    all_links.update(links)
                    all_links.add(url)
                    
                    time.sleep(0.5)  # Short delay
            
            except:
                pass
        
        self.log(f"\në°œê²¬ëœ HTML í˜ì´ì§€: {len(all_links)}ê°œ")
        
        if all_links:
            self.log("\në°œê²¬ëœ í˜ì´ì§€ ì˜ˆì‹œ:")
            for idx, link in enumerate(sorted(all_links)[:10], 1):
                filename = link.split('/')[-1]
                self.log(f"  {idx}. {filename}")
            if len(all_links) > 10:
                self.log(f"  ... ì™¸ {len(all_links) - 10}ê°œ")
        
        self.log(f"\n{'='*60}")
        self.log(f"2ë‹¨ê³„: ê° í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ {min(len(all_links), self.max_pages)}ê°œ)")
        self.log(f"{'='*60}\n")
        
        # Crawl each page - prioritize base URL first
        sorted_links = sorted(all_links)
        
        # Move base URL to front if it exists
        if self.base_url in sorted_links:
            sorted_links.remove(self.base_url)
            sorted_links.insert(0, self.base_url)
        
        for idx, url in enumerate(sorted_links[:self.max_pages], 1):
            if not self.should_continue():
                break
            
            if url in self.visited_urls:
                continue
            
            self.visited_urls.add(url)
            page_data = self.crawl_page(url)
            
            # Remove soup from stored data
            if 'soup' in page_data:
                del page_data['soup']
            
            self.pages_data.append(page_data)
            
            self.log(f"  ì§„í–‰: {idx}/{min(len(all_links), self.max_pages)}\n")
            
            if idx < len(sorted_links):
                time.sleep(self.delay)
        
        return self.pages_data
    
    def save_json(self):
        json_file = Path(self.output_dir, "crawlJson", "crawl_results.json")
        
        results = {
            'base_url': self.base_url,
            'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'summary': {
                'total_pages': len(self.pages_data),
                'successful_pages': sum(1 for p in self.pages_data if p['status'] == 'success'),
                'failed_pages': sum(1 for p in self.pages_data if p['status'] == 'error'),
            },
            'pages': self.pages_data
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return str(json_file)
    
    def save_txt(self):
        import re
        
        base_dir = Path(self.output_dir, "crawl_ì›ë¬¸")
        summary_file = Path(self.output_dir, "crawl_ìš”ì•½ë³¸", "í¬ë¡¤ë§_ìš”ì•½.txt")
        
        # Get current timestamp
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        # Save each page directly in crawl_ì›ë¬¸ folder
        saved_files = []
        for idx, page in enumerate(self.pages_data, 1):
            if page['status'] != 'success':
                continue
            
            # Clean title for filename
            title = page.get('title', 'Untitled')
            clean_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
            
            # Check if PDF
            file_type = page.get('file_type', 'html')
            type_marker = '[PDF]_' if file_type == 'pdf' else ''
            
            # Create filename: 001_[PDF]_Title_20240205_143022.txt
            filename = f"{idx:03d}_{type_marker}{clean_title}_{timestamp}.txt"
            filepath = base_dir / filename
            
            # Write individual file
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write(f"í˜ì´ì§€ {idx}: {title}\n")
                if file_type == 'pdf':
                    f.write("[PDF íŒŒì¼ì—ì„œ ë³€í™˜ë¨]\n")
                f.write("="*80 + "\n")
                f.write(f"URL: {page['url']}\n")
                f.write(f"í¬ë¡¤ë§ ì‹œê°„: {timestamp}\n")
                f.write(f"íŒŒì¼ í˜•ì‹: {file_type.upper()}\n")
                f.write("="*80 + "\n\n")
                
                if page.get('headings'):
                    f.write("ëª©ì°¨:\n" + "-"*80 + "\n")
                    for heading in page['headings']:
                        indent = "  " * (int(heading['level'][1]) - 1)
                        f.write(f"{indent}â€¢ {heading['text']}\n")
                    f.write("\n")
                
                if page.get('text'):
                    f.write("ë‚´ìš©:\n" + "-"*80 + "\n")
                    f.write(page['text'] + "\n\n")
                
                if page.get('code_blocks'):
                    f.write(f"ì½”ë“œ ë¸”ë¡ ({len(page['code_blocks'])}ê°œ):\n" + "-"*80 + "\n")
                    for code_idx, code in enumerate(page['code_blocks'], 1):
                        f.write(f"\n[ì½”ë“œ {code_idx}]\n{code}\n")
            
            saved_files.append(str(filepath))
        
        # Summary file (ëª©ë¡)
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("Doxygen ë¬¸ì„œ í¬ë¡¤ë§ ìš”ì•½\n")
            f.write("="*80 + "\n")
            f.write(f"ê¸°ì¤€ URL: {self.base_url}\n")
            f.write(f"í¬ë¡¤ë§ ë‚ ì§œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ í˜ì´ì§€: {len(self.pages_data)}\n")
            f.write(f"ì„±ê³µ: {sum(1 for p in self.pages_data if p['status'] == 'success')}\n")
            f.write(f"ì‹¤íŒ¨: {sum(1 for p in self.pages_data if p['status'] == 'error')}\n")
            f.write(f"HTML íŒŒì¼: {sum(1 for p in self.pages_data if p.get('file_type') == 'html')}\n")
            f.write(f"PDF íŒŒì¼: {sum(1 for p in self.pages_data if p.get('file_type') == 'pdf')}\n")
            f.write(f"ì €ì¥ëœ íŒŒì¼: {len(saved_files)}ê°œ\n")
            f.write("="*80 + "\n\n")
            
            f.write("ì €ì¥ëœ íŒŒì¼ ëª©ë¡:\n")
            f.write("-"*80 + "\n")
            for idx, page in enumerate(self.pages_data, 1):
                if page['status'] != 'success':
                    continue
                
                title = page.get('title', 'Untitled')
                clean_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
                file_type = page.get('file_type', 'html')
                type_marker = '[PDF]_' if file_type == 'pdf' else ''
                filename = f"{idx:03d}_{type_marker}{clean_title}_{timestamp}.txt"
                
                f.write(f"\n{idx}. {filename}\n")
                f.write(f"   ì œëª©: {title}\n")
                f.write(f"   í˜•ì‹: {file_type.upper()}\n")
                f.write(f"   URL: {page['url']}\n")
                
                if page.get('headings'):
                    f.write(f"   ì„¹ì…˜: {len(page['headings'])}ê°œ\n")
                
                if page.get('text'):
                    preview = page['text'][:200].replace('\n', ' ')
                    f.write(f"   ë¯¸ë¦¬ë³´ê¸°: {preview}...\n")
        
        return f"{len(saved_files)}ê°œ íŒŒì¼ ì €ì¥ë¨", str(summary_file)


def main():
    root = tk.Tk()
    app = DoxygenCrawlerGUI(root)
    root.mainloop()


if __name__ == '__main__':
    main()
