"""Unified Crawler CLI - Command Line Interface version."""

import os
import sys
import subprocess
import time
import argparse
from pathlib import Path
from urllib.parse import urlparse


class CrawlerCLI:
    """CLI launcher for selecting and running crawlers."""
    
    def __init__(self):
        self.is_crawling = False
        self.process = None
    
    def run(self, args):
        """Run crawler based on arguments."""
        # Validate inputs
        if not args.url:
            print("âŒ Error: URL is required")
            return 1
        
        # Check prerequisites
        ok, error_msg = self._check_prerequisites(args.crawler_type)
        if not ok:
            print(f"âŒ Error: {error_msg}")
            return 1
        
        # Convert output dir to absolute path
        output_dir = os.path.abspath(args.output_dir)
        
        # Run appropriate crawler
        if args.crawler_type == "simple":
            return self._run_simple_crawler(
                args.url, 
                args.max_pages, 
                args.delay, 
                output_dir
            )
        else:
            return self._run_advanced_crawler(
                args.url,
                args.max_pages,
                output_dir,
                args.depth,
                args.render
            )
    
    def _check_prerequisites(self, crawler_type):
        """Check if required tools are installed."""
        if crawler_type == "simple":
            # Check Python packages
            try:
                import requests
                import bs4
                return True, ""
            except ImportError as e:
                return False, f"í•„ìˆ˜ íŒ¨í‚¤ì§€ ëˆ„ë½: {e.name}\nì‹¤í–‰: pip install requests beautifulsoup4"
        
        else:  # advanced
            # Check Scrapy
            try:
                result = subprocess.run(["scrapy", "version"], capture_output=True, text=True)
                if result.returncode != 0:
                    return False, "Scrapyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nsetup_advanced.batì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!"
                return True, ""
            except FileNotFoundError:
                return False, "Scrapyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nsetup_advanced.batì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!"
            except Exception as e:
                return False, f"Scrapy í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _run_simple_crawler(self, url, max_pages, delay, output_dir):
        """Run simple crawler."""
        try:
            print("="*60)
            print("ê°„ë‹¨ í¬ë¡¤ëŸ¬ ì‹œì‘")
            print("="*60)
            print(f"URL: {url}")
            print(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}")
            print(f"ì¶œë ¥: {output_dir}")
            print("")
            
            # Import and run simple crawler
            sys.path.insert(0, str(Path(__file__).parent / "simple_crawler"))
            from crawler import DoxygenCrawler
            
            def log_func(msg):
                print(msg)
            
            def should_continue():
                return True
            
            crawler = DoxygenCrawler(
                url, max_pages, delay, output_dir,
                log_func, should_continue
            )
            results = crawler.crawl()
            
            if results:
                print(f"\n{'='*60}")
                print("ê²°ê³¼ ì €ì¥ ì¤‘...")
                
                json_file = crawler.save_json()
                files_msg = crawler.save_txt()
                
                print(f"âœ“ JSONL: {json_file}")
                print(f"âœ“ TXT íŒŒì¼: {files_msg}")
                print(f"{'='*60}\n")
                print(f"âœ… ì™„ë£Œ! ì´ {len(results)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")
                print(f"\nğŸ“ ì¶œë ¥ ìœ„ì¹˜:")
                print(f"  - TXT: {output_dir}/simple_crawler/")
                print(f"  - JSON: {output_dir}/simple_json/pages.jsonl")
                
                return 0
            else:
                print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
                return 1
        
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return 1
    
    def _run_advanced_crawler(self, url, max_pages, output_dir, depth, render):
        """Run advanced Scrapy crawler."""
        try:
            # Extract domain from URL
            parsed = urlparse(url)
            domain = parsed.netloc
            
            print("="*60)
            print("ê³ ê¸‰ í¬ë¡¤ëŸ¬ (Scrapy) ì‹œì‘")
            print("="*60)
            print(f"URL: {url}")
            print(f"ë„ë©”ì¸: {domain}")
            print(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}")
            print(f"ê¹Šì´: {depth}")
            print(f"ë Œë”ë§: {'ì‚¬ìš©' if render else 'ì‚¬ìš© ì•ˆ í•¨'}")
            print(f"ì¶œë ¥: {output_dir}")
            print("")
            
            # Build Scrapy command
            scrapy_dir = Path(__file__).parent / "scrapy_crawler"
            
            cmd = [
                "scrapy", "crawl", "site",
                "-a", f"seed={url}",
                "-a", f"allowed_domains={domain}",
                "-a", f"out_dir={output_dir}",
                "-a", f"max_pages={max_pages}",
                "-a", f"max_depth={depth}",
                "-a", f"render={1 if render else 0}"
            ]
            
            # Run Scrapy with real-time output
            self.process = subprocess.Popen(
                cmd,
                cwd=str(scrapy_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Stream output with smart idle detection
            last_activity = time.time()
            max_idle_time = 180  # 3ë¶„ê°„ ì§„í–‰ ì—†ìœ¼ë©´ ê°•ì œ ì¢…ë£Œ
            last_page_count = 0
            
            # ìˆ¨ê¸¸ ë¡œê·¸ íŒ¨í„´ (ë” í¬ê´„ì ìœ¼ë¡œ)
            skip_patterns = [
                '[asyncio] ERROR',
                'AssertionError',
                'ScrapyDeprecationWarning',
                '[py.warnings] WARNING',
                'Traceback',
                'File "',
                'self._context.run',
                'assert f is self._write_fut',
                'handle: <Handle',
                '~~~~~~~~~~~~~~~~~',
                '^^^^^^^^^^^^',
                '[readability.readability] INFO',
                'ruthless removal',
                '_ProactorBaseWritePipeTransport',
                '_loop_writing()',
                'INFO: Scrapy',
                'INFO: Versions',
                'INFO: Enabled',
                'Telnet',
                'Started loop on separate thread',
                'download handler',
                'spider middlewares',
                'downloader middlewares',
                'item pipelines',
                'Overridden settings',
                "'lxml':",
                "'libxml2':",
                "'cssselect':",
                "'parsel':",
                "'w3lib':",
                "'Twisted':",
                "'Python':",
                "'pyOpenSSL':",
                "'cryptography':",
                "'Platform':"
            ]
            
            for line in iter(self.process.stdout.readline, ''):
                # ë¶ˆí•„ìš”í•œ ë¡œê·¸ í•„í„°ë§
                if any(pattern in line for pattern in skip_patterns):
                    continue
                
                # ë¹ˆ ì¤„ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ì¤„ë„ ìŠ¤í‚µ
                if not line.strip():
                    continue
                
                print(line.rstrip())
                
                # Check for ACTUAL progress (page count increasing)
                if 'Crawled' in line and 'pages' in line:
                    try:
                        # Extract page count: "Crawled 16 pages"
                        parts = line.split('Crawled')[1].split('pages')[0].strip()
                        current_pages = int(parts)
                        
                        # Only reset timer if pages increased
                        if current_pages > last_page_count:
                            last_activity = time.time()
                            last_page_count = current_pages
                    except:
                        pass
                
                # Also reset on other important events
                if 'Spider opened' in line or 'Launching browser' in line or '[âœ“]' in line:
                    last_activity = time.time()
                
                # Check if closing (give it 30 seconds to finish)
                if 'Closing spider' in line:
                    print("\nâš ï¸  Spider ì¢…ë£Œ ì¤‘... 30ì´ˆ ëŒ€ê¸°")
                    max_idle_time = 30  # Reduce timeout when closing
                
                # Force kill if idle too long
                idle_time = time.time() - last_activity
                if idle_time > max_idle_time:
                    print(f"\nâš ï¸  {int(idle_time)}ì´ˆê°„ ì§„í–‰ ì—†ìŒ. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                    self.process.kill()
                    break
            
            # Wait for process to finish
            try:
                self.process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                print("\nâš ï¸  í”„ë¡œì„¸ìŠ¤ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŒ. ê°•ì œ ì¢…ë£Œ...")
                self.process.kill()
                self.process.wait()
            
            if self.process.returncode == 0:
                print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
                print(f"\nğŸ“ ì¶œë ¥:")
                print(f"  - TXT: {output_dir}/scrapy_crawler/")
                print(f"  - JSON: {output_dir}/scrapy_json/pages.jsonl")
                return 0
            elif self.process.returncode is None:
                # Process killed due to timeout
                print("\nâš ï¸  í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œë¨ (íƒ€ì„ì•„ì›ƒ)")
                print(f"\nğŸ“ ì¶œë ¥:")
                print(f"  - TXT: {output_dir}/scrapy_crawler/")
                print(f"  - JSON: {output_dir}/scrapy_json/pages.jsonl")
                return 0
            else:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ì½”ë“œ: {self.process.returncode})")
                return 1
        
        except FileNotFoundError:
            print("\nâŒ Scrapyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\nsetup_advanced.batì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!")
            return 1
        
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return 1


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="í†µí•© ì›¹ í¬ë¡¤ëŸ¬ - CLI ë²„ì „",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê°„ë‹¨ í¬ë¡¤ëŸ¬ ì‚¬ìš©
  python launcher_CLI.py -t simple -u "https://example.com/docs/index.html" -m 100

  # ê³ ê¸‰ í¬ë¡¤ëŸ¬ ì‚¬ìš© (ë Œë”ë§ ì—†ìŒ)
  python launcher_CLI.py -t advanced -u "https://vertx.io/docs/" -m 50 --no-render

  # ì¶œë ¥ í´ë” ì§€ì •
  python launcher_CLI.py -t simple -u "https://example.com" -o ./my_output
        """
    )
    
    # Required arguments
    parser.add_argument(
        "-t", "--type",
        dest="crawler_type",
        choices=["simple", "advanced"],
        required=True,
        help="í¬ë¡¤ëŸ¬ íƒ€ì…: simple (ë¹ ë¦„, ì •ì  HTML) ë˜ëŠ” advanced (ëŠë¦¼, SPA)"
    )
    
    parser.add_argument(
        "-u", "--url",
        required=True,
        help="ì‹œì‘ URL"
    )
    
    # Optional arguments
    parser.add_argument(
        "-m", "--max-pages",
        type=int,
        default=500,
        help="ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 500)"
    )
    
    parser.add_argument(
        "-o", "--output",
        dest="output_dir",
        default="./crawl_output",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./crawl_output)"
    )
    
    # Simple crawler options
    parser.add_argument(
        "-d", "--delay",
        type=float,
        default=1.0,
        help="ìš”ì²­ ê°„ê²© (ì´ˆ, ê°„ë‹¨ í¬ë¡¤ëŸ¬ë§Œ í•´ë‹¹, ê¸°ë³¸ê°’: 1.0)"
    )
    
    # Advanced crawler options
    parser.add_argument(
        "--depth",
        type=int,
        default=4,
        help="ìµœëŒ€ ê¹Šì´ (ê³ ê¸‰ í¬ë¡¤ëŸ¬ë§Œ í•´ë‹¹, ê¸°ë³¸ê°’: 4)"
    )
    
    parser.add_argument(
        "--no-render",
        dest="render",
        action="store_false",
        default=True,
        help="Playwright ë Œë”ë§ ë¹„í™œì„±í™” (ê³ ê¸‰ í¬ë¡¤ëŸ¬ë§Œ í•´ë‹¹)"
    )
    
    # Version
    parser.add_argument(
        "-v", "--version",
        action="version",
        version="%(prog)s 2.0"
    )
    
    args = parser.parse_args()
    
    # Run crawler
    cli = CrawlerCLI()
    exit_code = cli.run(args)
    sys.exit(exit_code)


if __name__ == '__main__':
    main()
